# PySpark-Tutorial
My self-learning about PySpark

## 1. Spark streaming
Read my own document about `Spark streaming` [here](https://docs.google.com/document/d/1PkBWu7exb7gZz-5HfubrwusVSdKrKPcr/edit#heading=h.49x2ik5).

## 2. Spark Streaming Working Principles

![plot](https://github.com/DatacollectorVN/PySpark-Tutorial/blob/master/public-imgs/spark_streaming_working_principle.png?raw=true)

Read my own document [here](https://docs.google.com/document/d/1PkBWu7exb7gZz-5HfubrwusVSdKrKPcr/edit#heading=h.am1d9vgdpgwx)

## Quickly example:
Source [here](https://spark.apache.org/docs/latest/streaming-programming-guide.html)

Read carefully file `2.streaming_network_wordcount.py`, we explained detail.

For running structure streaming, you need run 2 terminal:

**First terminal**: running **Netcat server** for simulating the `socket resource` via port `9999`.
```bash
nc -lk 9999
```

**Second terminal**: running structure streaming. You have 2 ways:

*Running without submit `PySpark` application program (or job) to the `cluster manager`*:
```bash
python 2.streaming_network_wordcount.py localhost 9999
```

*Note:* You must running **Netcat server** before running structure streaming.

*Submit `PySpark` application program (or job) to the `cluster manager`*:

First of all, check the `bin/spark-submit` location, in our case, we dowloaded `apache spark` via `homebrew`. Therefore, it located at `/opt/homebrew/Cellar/apache-spark/3.2.1/bin/spark-submit`.

Then running:
```bash
/opt/homebrew/Cellar/apache-spark/3.2.1/bin/spark-submit 2.streaming_network_wordcount.py localhost 9999
```

**Note:** You can compare the results with the `1.structure_streaming` to make clear.

## 3. Discretized Stream (Dstream)

Read my own document about Dstream [here](https://docs.google.com/document/d/1PkBWu7exb7gZz-5HfubrwusVSdKrKPcr/edit#heading=h.2xp6fwjot5r7)

**In quickly example:**

The `line` is `DStream`.
```bash
lines = ssc.socketTextStream("localhost", 9999)
```

The `words` is `DStream` too. That is generated by `flatMap` methods.
```bash
words = lines.flatMap(lambda line: line.split(" "))
```

![plot](https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)


## 4. Initializing StreamingContext
To initialize a `Spark Streaming` program, a `StreamingContext` object has to be created which is the main entry point of all Spark Streaming functionality.

```bash
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext(<master>, <appName>)
ssc = StreamingContext(sc, 1)
```

The `appName` parameter is a name for your application to show on the cluster UI. 

The `master` is a `Spark`, `Mesos` or `YARN` cluster URL, or a special `local[*]` string to run in **local mode**. 

In practice, when running on a cluster, **you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there**. However, for local testing and unit tests, you can pass `local[*]` to run Spark Streaming in-process (detects the number of cores in the local system).

**In Quickly example**
```bash
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1) # 1 mean batch interval of 1s.
```

**After a context is defined, you have to do the following:**
- Define the input sources by creating input `DStreams`.
- Define the streaming computations by applying transformation and output operations to `DStreams`.
- Start receiving data and processing it using `streamingContext.start()`.
- Wait for the processing to be stopped (manually or due to any error) using `streamingContext.awaitTermination()`.
- The processing can be manually stopped using `streamingContext.stop()`.

**Points to remember:**
- **Once a context has been started, no new streaming computations can be set up or added to it**.
- **Once a context has been stopped, it cannot be restarted**.
- **Only one `StreamingContext` can be active in a JVM at the same time**.
- ``stop()`` on `StreamingContext` also stops the `SparkContext`. To stop only the `StreamingContext`, set the optional parameter of stop() called stopSparkContext to false `ssc.stop(stopSparkContext=False)`.
- A `SparkContext` can be re-used to create multiple `StreamingContexts`, as long as the previous `StreamingContext` is stopped (without stopping the `SparkContext`) before the next `StreamingContext` is created.

## 5. `Input DStreams` and `Receivers`
`Input DStreams` are `DStreams` representing the stream of input data received from streaming sources.

**In Quickly example**
`lines` was an input DStream as it represented the stream of data received from the netcat server.

```bash
lines = ssc.socketTextStream("localhost", 9999)
```

Every `input DStream` (except file stream) is associated with a `Receiver` (Scala doc, Java doc) object which receives the data from a source and stores it in Sparkâ€™s memory for processing. See the figure in Section 2 `Spark Streaming Working Principles`.

`Spark Streaming` provides two categories of `built-in streaming sources`:
- *Basic sources*: Sources directly available in the `StreamingContext` API. Examples: file systems, and socket connections.
- *Advanced sources*: Sources like `Kafka`, `Kinesis`, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the [linking](https://spark.apache.org/docs/latest/streaming-programming-guide.html#linking) section.

**Multiple sources:**
if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams. 
read some examples [here](https://spark.apache.org/docs/latest/streaming-programming-guide.html#level-of-parallelism-in-data-receiving).

**Points to remember:**
- When running a `Spark Streaming` program locally, do not use `local` or `local[1]` as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input `DStream` based on a receiver (e.g. `sockets`, `Kafka`, etc.), then the single thread will be used to run the `receiver`, leaving no thread for processing the received data. Hence, when running locally, always use `local[n]` as the master URL, *where n > number of receivers to run (see Spark Properties [here](https://spark.apache.org/docs/latest/configuration.html#spark-properties) for information on how to set the master)*.

- Extending the logic to running on a cluster, the number of cores allocated to the `Spark Streaming` application must be more than the number of `receivers`. Otherwise the system will receive data, but not be able to process it.

## 6. Basic Sources
`ssc.socketTextStream(...)` in the quick example which creates a `DStream` from text data received over a `TCP socket connection`. Besides sockets, the `StreamingContext` API provides methods for creating `DStreams` from files as input sources.

**File Streams**

For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as via `StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]`.

File streams do not require running a receiver so there is no need to allocate any cores for receiving file data.

For simple text files, the easiest method is `StreamingContext.textFileStream(dataDirectory)`.
`fileStream` is not available in the `Python API`, only `textFileStream` is available.
```bash
streamingContext.textFileStream(dataDirectory)
```