# PySpark-Tutorial
My self-learning about PySpark

## 1. Spark streaming
Read my own document about `Spark streaming` [here](https://docs.google.com/document/d/1PkBWu7exb7gZz-5HfubrwusVSdKrKPcr/edit#heading=h.49x2ik5).

## 2. Spark Streaming Working Principles

![plot](https://github.com/DatacollectorVN/PySpark-Tutorial/blob/master/public-imgs/spark_streaming_working_principle.png?raw=true)

Read my own document [here](https://docs.google.com/document/d/1PkBWu7exb7gZz-5HfubrwusVSdKrKPcr/edit#heading=h.am1d9vgdpgwx)

## Quickly example:
Source [here](https://spark.apache.org/docs/latest/streaming-programming-guide.html)

Read carefully file `2.streaming_network_wordcount.py`, we explained detail.

For running structure streaming, you need run 2 terminal:

**First terminal**: running **Netcat server** for simulating the `socket resource` via port `9999`.
```bash
nc -lk 9999
```

**Second terminal**: running structure streaming. You have 2 ways:

*Running without submit `PySpark` application program (or job) to the `cluster manager`*:
```bash
python 2.streaming_network_wordcount.py localhost 9999
```

*Note:* You must running **Netcat server** before running structure streaming.

*Submit `PySpark` application program (or job) to the `cluster manager`*:

First of all, check the `bin/spark-submit` location, in our case, we dowloaded `apache spark` via `homebrew`. Therefore, it located at `/opt/homebrew/Cellar/apache-spark/3.2.1/bin/spark-submit`.

Then running:
```bash
/opt/homebrew/Cellar/apache-spark/3.2.1/bin/spark-submit 2.streaming_network_wordcount.py localhost 9999
```

**Note:** You can compare the results with the `1.structure_streaming` to make clear.

## 3. Discretized Stream (Dstream)

Read my own document about Dstream [here](https://docs.google.com/document/d/1PkBWu7exb7gZz-5HfubrwusVSdKrKPcr/edit#heading=h.2xp6fwjot5r7)

**In quickly example:**

The `line` is `DStream`.
```bash
lines = ssc.socketTextStream("localhost", 9999)
```

The `words` is `DStream` too. That is generated by `flatMap` methods.
```bash
words = lines.flatMap(lambda line: line.split(" "))
```

![plot](https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)


## 4. Initializing StreamingContext
To initialize a `Spark Streaming` program, a `StreamingContext` object has to be created which is the main entry point of all Spark Streaming functionality.

```bash
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext(<master>, <appName>)
ssc = StreamingContext(sc, 1)
```

The `appName` parameter is a name for your application to show on the cluster UI. 

The `master` is a `Spark`, `Mesos` or `YARN` cluster URL, or a special `local[*]` string to run in **local mode**. 

In practice, when running on a cluster, **you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there**. However, for local testing and unit tests, you can pass `local[*]` to run Spark Streaming in-process (detects the number of cores in the local system).

**In Quickly example**
```bash
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1) # 1 mean batch interval of 1s.
```

**After a context is defined, you have to do the following:**
- Define the input sources by creating input `DStreams`.
- Define the streaming computations by applying transformation and output operations to `DStreams`.
- Start receiving data and processing it using `streamingContext.start()`.
- Wait for the processing to be stopped (manually or due to any error) using `streamingContext.awaitTermination()`.
- The processing can be manually stopped using `streamingContext.stop()`.

**Points to remember:**
- **Once a context has been started, no new streaming computations can be set up or added to it**.
- **Once a context has been stopped, it cannot be restarted**.
- **Only one `StreamingContext` can be active in a JVM at the same time**.
- ``stop()`` on `StreamingContext` also stops the `SparkContext`. To stop only the `StreamingContext`, set the optional parameter of stop() called stopSparkContext to false `ssc.stop(stopSparkContext=False)`.
- A `SparkContext` can be re-used to create multiple `StreamingContexts`, as long as the previous `StreamingContext` is stopped (without stopping the `SparkContext`) before the next `StreamingContext` is created.