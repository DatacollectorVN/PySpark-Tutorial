{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "import os\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 11:54:28 WARN Utils: Your hostname, Nathans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.255.49.235 instead (on interface en0)\n",
      "22/05/09 11:54:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/09 11:54:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/09 11:54:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('..', 'src-data', '2017_StPaul_MN_Real_Estate.csv')\n",
    "df = spark.read.csv(file_path, header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.select('STREETNUMBERNUMERIC', 'FIREPLACES', \n",
    "                   'LOTSIZEDIMENSIONS', 'LISTTYPE', 'ACRES', \n",
    "                   'ASSUMABLEMORTGAGE', 'SalesClosePrice', 'ListPrice',\n",
    "                   'DAYSONMARKET', 'YEARBUILT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fundamental Mathematic Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn ('TSQT', (df['SQFTBELOWGROUND'] + df['SQFTABOVEGROUND']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('BED_TO_BATHS', \n",
    "                  df['Bedrooms'] / df['BathsFull'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|BED_TO_BATHS|\n",
      "+------------+\n",
      "|         3.0|\n",
      "|         4.0|\n",
      "|         2.0|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['BED_TO_BATHS'].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regular Expression\n",
    "Use `regexp_extract`. Read [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.regexp_extract.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Time Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to date type\n",
    "func = F.udf(lambda x: datetime.strptime(x, \"%m/%d/%Y %H:%M\"), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Date type\n",
    "df = df.withColumn(\"ListDateNew\", func(F.col(\"LISTDATE\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date\n",
    "df = df.withColumn(\"ListDateNew_1\", F.to_date(\"ListDateNew\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get day of week\n",
    "df = df.withColumn(\"List_day_week\", F.dayofweek(\"ListDateNew_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year\n",
    "df = df.withColumn(\"Year\", F.year(\"ListDateNew_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+----+\n",
      "|ListDateNew|ListDateNew_1|List_day_week|Year|\n",
      "+-----------+-------------+-------------+----+\n",
      "| 2017-07-15|   2017-07-15|            7|2017|\n",
      "| 2017-10-09|   2017-10-09|            2|2017|\n",
      "| 2017-06-26|   2017-06-26|            2|2017|\n",
      "| 2017-08-25|   2017-08-25|            6|2017|\n",
      "| 2017-09-12|   2017-09-12|            3|2017|\n",
      "+-----------+-------------+-------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['ListDateNew', 'ListDateNew_1', 'List_day_week', 'Year']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_attached_garage = df[\"GARAGEDESCRIPTION\"].like(\"%Attached Garage%\")\n",
    "has_detached_garage = df[\"GARAGEDESCRIPTION\"].like(\"%Detached Garage%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"has_attached_garage\", (F.when(has_attached_garage, 1)\n",
    "                                           .when(has_detached_garage, 0)\n",
    "                                           .otherwise(None)\n",
    "                                          )\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-------------------+\n",
      "|                                 GARAGEDESCRIPTION|has_attached_garage|\n",
      "+--------------------------------------------------+-------------------+\n",
      "|                                   Attached Garage|                  1|\n",
      "|Attached Garage, Driveway - Asphalt, Garage Doo...|                  1|\n",
      "|                                   Attached Garage|                  1|\n",
      "|Attached Garage, Detached Garage, Tuckunder, Dr...|                  1|\n",
      "|Attached Garage, Driveway - Asphalt, Garage Doo...|                  1|\n",
      "+--------------------------------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[[\"GARAGEDESCRIPTION\", \"has_attached_garage\"]].show(truncate = 50, n= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split` function: split string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = F.split(df[\"ROOF\"], \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Roof_Material\", split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+----------------+\n",
      "|                                          ROOF|   Roof_Material|\n",
      "+----------------------------------------------+----------------+\n",
      "|                                          null|            null|\n",
      "|Asphalt Shingles, Pitched, Age 8 Years or Less|Asphalt Shingles|\n",
      "|                                          null|            null|\n",
      "|Asphalt Shingles, Pitched, Age 8 Years or Less|Asphalt Shingles|\n",
      "|            Asphalt Shingles, Age Over 8 Years|Asphalt Shingles|\n",
      "+----------------------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[[\"ROOF\", \"Roof_Material\"]].show(5, truncate = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Explode` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"roof_list\", F.split(df[\"ROOF\"], \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "| No|                                         roof_list|\n",
      "+---+--------------------------------------------------+\n",
      "|  1|                                              null|\n",
      "|  2|[Asphalt Shingles,  Pitched,  Age 8 Years or Less]|\n",
      "|  3|                                              null|\n",
      "|  4|[Asphalt Shingles,  Pitched,  Age 8 Years or Less]|\n",
      "|  5|             [Asphalt Shingles,  Age Over 8 Years]|\n",
      "+---+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[[\"No\", \"roof_list\"]].show(5, truncate = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'roof_list' feature contain many value by each record.\n",
    "\n",
    "Convert it to `1NF` by `explode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "roof_df = df.withColumn(\"ex_roof_list\", F.explode(df[\"roof_list\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| No|        ex_roof_list|\n",
      "+---+--------------------+\n",
      "|  2|    Asphalt Shingles|\n",
      "|  2|             Pitched|\n",
      "|  2| Age 8 Years or Less|\n",
      "|  4|    Asphalt Shingles|\n",
      "|  4|             Pitched|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roof_df[[\"No\", \"ex_roof_list\"]].show(5, truncate = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the column `ex_roof_list`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy by `lit` function. Read [here](https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy column of constant value\n",
    "roof_df = roof_df.withColumn(\"constant_val\", F.lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| No|        ex_roof_list|constant_val|\n",
      "+---+--------------------+------------+\n",
      "|  2|    Asphalt Shingles|           1|\n",
      "|  2|             Pitched|           1|\n",
      "|  2| Age 8 Years or Less|           1|\n",
      "|  4|    Asphalt Shingles|           1|\n",
      "|  4|             Pitched|           1|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roof_df[[\"No\", \"ex_roof_list\", \"constant_val\"]].show(5, truncate = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use `first` (read [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.first.html)) and `coalesce` (read [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.coalesce.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roof_piv_df = roof_df.groupBy(\"No\").pivot(\"ex_roof_list\").agg(F.coalesce(F.first(\"constant_val\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 12:16:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+-----+------+------+--------+-------+-------+------+-----------+-----+--------------------+--------------+-------------------+----------------+----------------+----+-----+-----+-------+------+------+-----+----------+----+-------------------+-------------+\n",
      "| No| Age 8 Years or Less| Age Over 8 Years| Flat| Metal| Other| Pitched| Rubber| Shakes| Slate| Tar/Gravel| Tile| Unspecified Shingle| Wood Shingles|Age 8 Years or Less|Age Over 8 Years|Asphalt Shingles|Flat|Metal|Other|Pitched|Rubber|Shakes|Slate|Tar/Gravel|Tile|Unspecified Shingle|Wood Shingles|\n",
      "+---+--------------------+-----------------+-----+------+------+--------+-------+-------+------+-----------+-----+--------------------+--------------+-------------------+----------------+----------------+----+-----+-----+-------+------+------+-----+----------+----+-------------------+-------------+\n",
      "|  2|                   1|             null| null|  null|  null|       1|   null|   null|  null|       null| null|                null|          null|               null|            null|               1|null| null| null|   null|  null|  null| null|      null|null|               null|         null|\n",
      "|  4|                   1|             null| null|  null|  null|       1|   null|   null|  null|       null| null|                null|          null|               null|            null|               1|null| null| null|   null|  null|  null| null|      null|null|               null|         null|\n",
      "|  5|                null|                1| null|  null|  null|    null|   null|   null|  null|       null| null|                null|          null|               null|            null|               1|null| null| null|   null|  null|  null| null|      null|null|               null|         null|\n",
      "|  6|                   1|             null| null|  null|  null|    null|   null|   null|  null|       null| null|                null|          null|               null|            null|               1|null| null| null|   null|  null|  null| null|      null|null|               null|         null|\n",
      "|  7|                   1|             null| null|  null|  null|    null|   null|   null|  null|       null| null|                null|          null|               null|            null|               1|null| null| null|   null|  null|  null| null|      null|null|               null|         null|\n",
      "+---+--------------------+-----------------+-----+------+------+--------+-------+-------+------+-----------+-----+--------------------+--------------+-------------------+----------------+----------------+----+-----+-----+-------+------+------+-----+----------+----+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roof_piv_df.orderBy(\"No\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data = df.join(roof_piv_df, on = \"No\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of column 108\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of column {len(joined_data.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero fill column of root_piv_dif\n",
    "zfill_cols = roof_piv_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No',\n",
       " ' Age 8 Years or Less',\n",
       " ' Age Over 8 Years',\n",
       " ' Flat',\n",
       " ' Metal',\n",
       " ' Other',\n",
       " ' Pitched',\n",
       " ' Rubber',\n",
       " ' Shakes',\n",
       " ' Slate',\n",
       " ' Tar/Gravel',\n",
       " ' Tile',\n",
       " ' Unspecified Shingle',\n",
       " ' Wood Shingles',\n",
       " 'Age 8 Years or Less',\n",
       " 'Age Over 8 Years',\n",
       " 'Asphalt Shingles',\n",
       " 'Flat',\n",
       " 'Metal',\n",
       " 'Other',\n",
       " 'Pitched',\n",
       " 'Rubber',\n",
       " 'Shakes',\n",
       " 'Slate',\n",
       " 'Tar/Gravel',\n",
       " 'Tile',\n",
       " 'Unspecified Shingle',\n",
       " 'Wood Shingles']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zfill_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfilled_df = joined_data.fillna(0, subset = zfill_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Binarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Binarizing` is technique to create new feature from existing feature into binary format (0 or 1) with threshold.\n",
    "\n",
    "Read [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Binarizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"SalesClosePrice\", df[\"SalesClosePrice\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_price = df.agg({\"SalesClosePrice\": \"mean\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = feature.Binarizer(threshold = mean_price, inputCol = \"SalesClosePrice\", outputCol = \"newSalesClosePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform binary to DataFrame\n",
    "df_new = binary.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|SalesClosePrice|newSalesClosePrice|\n",
      "+---------------+------------------+\n",
      "|       143000.0|               0.0|\n",
      "|       190000.0|               0.0|\n",
      "|       225000.0|               0.0|\n",
      "|       265000.0|               1.0|\n",
      "|       249900.0|               0.0|\n",
      "|       255000.0|               0.0|\n",
      "+---------------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new[[\"SalesClosePrice\", \"newSalesClosePrice\"]].show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bucketing:\n",
    "`Bucketing` is a technique in both Spark and Hive used to optimize the performance of the task. In bucketing buckets (clustering columns) determine data partitioning and prevent data shuffle. Based on the value of one or more bucketing columns, the data is allocated to a predefined number of buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](https://miro.medium.com/max/1164/0*eXB5TGP81IzwJHvh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more [here](https://blog.clairvoyantsoft.com/bucketing-in-spark-878d2e02140f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in `PySpark` use `Bucketizer`. Read [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0,1,2,3,4,float(\"Inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "buck = feature.Bucketizer(splits =splits, inputCol = \"BATHSTOTAL\", outputCol = \"baths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_2 = buck.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|BATHSTOTAL|baths|\n",
      "+----------+-----+\n",
      "|         2|  2.0|\n",
      "|         3|  3.0|\n",
      "|         1|  1.0|\n",
      "|         2|  2.0|\n",
      "|         2|  2.0|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_2[[\"BATHSTOTAL\", \"baths\"]].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [(0.1, 0.0), (0.4, 1.0), (1.2, 1.3), (1.5, float(\"nan\")), (float(\"nan\"), 1.0), (float(\"nan\"), 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark.createDataFrame(values, [\"values1\", \"values2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|values1|values2|\n",
      "+-------+-------+\n",
      "|    0.1|    0.0|\n",
      "|    0.4|    1.0|\n",
      "|    1.2|    1.3|\n",
      "|    1.5|    NaN|\n",
      "|    NaN|    1.0|\n",
      "|    NaN|    0.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "buck = feature.Bucketizer(splits = [-float(\"inf\"), 0.5, 1.4, float(\"inf\")], inputCol = \"values1\", outputCol = \"baths_values1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_2 = buck.setHandleInvalid(\"keep\").transform(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+\n",
      "|values1|values2|baths_values1|\n",
      "+-------+-------+-------------+\n",
      "|    0.1|    0.0|          0.0|\n",
      "|    0.4|    1.0|          0.0|\n",
      "|    1.2|    1.3|          1.0|\n",
      "|    1.5|    NaN|          2.0|\n",
      "|    NaN|    1.0|          3.0|\n",
      "+-------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. One hot encoding\n",
    "`One hot encoding`: is technique ecnoding feature into one-hoe numeric array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark.createDataFrame([\n",
    "    (\"a\", 2.0),\n",
    "    (\"b\", 2.0),\n",
    "    (\"c\", 1.0),\n",
    "    (\"a\", 1.0),\n",
    "    (\"a\", 0.0),\n",
    "    (\"b\", 1.0),\n",
    "    (\"c\", 1.0)\n",
    "], [\"categoryIndex\", \"categoryIndex2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|categoryIndex|categoryIndex2|\n",
      "+-------------+--------------+\n",
      "|            a|           2.0|\n",
      "|            b|           2.0|\n",
      "|            c|           1.0|\n",
      "|            a|           1.0|\n",
      "|            a|           0.0|\n",
      "|            b|           1.0|\n",
      "|            c|           1.0|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StringIndexer`: It's like `label-encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringID = feature.StringIndexer(inputCol=\"categoryIndex\", outputCol=\"categoryIndex1\") \n",
    "indexer = stringID.fit(df_temp)\n",
    "df_after = indexer.transform(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|categoryIndex|categoryIndex1|\n",
      "+-------------+--------------+\n",
      "|            a|           0.0|\n",
      "|            b|           1.0|\n",
      "|            c|           2.0|\n",
      "|            a|           0.0|\n",
      "|            a|           0.0|\n",
      "|            b|           1.0|\n",
      "|            c|           2.0|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_after[['categoryIndex', 'categoryIndex1']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OneHotEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = feature.OneHotEncoder(inputCol=\"categoryIndex1\",\n",
    "                        outputCol=\"categoryVec1\", \n",
    "                        dropLast=True) # default\n",
    "encoded = encoder.fit(df_after)\n",
    "df_after = encoded.transform(df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+-------------+\n",
      "|categoryIndex|categoryIndex1| categoryVec1|\n",
      "+-------------+--------------+-------------+\n",
      "|            a|           0.0|(2,[0],[1.0])|\n",
      "|            b|           1.0|(2,[1],[1.0])|\n",
      "|            c|           2.0|    (2,[],[])|\n",
      "|            a|           0.0|(2,[0],[1.0])|\n",
      "|            a|           0.0|(2,[0],[1.0])|\n",
      "|            b|           1.0|(2,[1],[1.0])|\n",
      "|            c|           2.0|    (2,[],[])|\n",
      "+-------------+--------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 13:14:24 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 164967 ms exceeds timeout 120000 ms\n",
      "22/05/09 13:14:24 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "df_after[[\"categoryIndex\", \"categoryIndex1\", \"categoryVec1\"]].show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8b2dc75a532c1b8be16e00f9f13abc240a1ac4ad83b862d6ae36ef062a2a95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
