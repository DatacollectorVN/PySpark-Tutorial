{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "import os\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/09 12:50:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/09 12:50:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/09 12:50:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('..', 'src-data', 'AA_data')\n",
    "df = spark.read.csv(file_path, header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save DataFrame in memory or disk.\n",
    "- Improve velocity on transformation / action.\n",
    "- Reduce source when using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Don't caching it:\n",
    "- Large dataset.\n",
    "- local disk based on caching might be not the way for imporving productivities.\n",
    "- Non-avaiable objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cache by `DF_name`.cache().`action()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "583718"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether the object is caching or not.\n",
    "\n",
    "Use `is_cached`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.is_cached: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"df.is_cached: {df.is_cached}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove caching.\n",
    "\n",
    "Use `unpersist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.is_cached: False\n"
     ]
    }
   ],
   "source": [
    "df.unpersist()\n",
    "print(f\"df.is_cached: {df.is_cached}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parquet` is an open source file format built to handle flat columnar storage data formats. `Parquet` operates well with complex data in large volumes.It is known for its both performant data compression and its ability to handle a wide variety of encoding types. \n",
    "\n",
    "`Parquet` deploys Google's record-shredding and assembly algorithm that can address complex data structures within data storage. Some \n",
    "`Parquet` benefits include:\n",
    "\n",
    "- Fast queries that can fetch specific column values without reading full row data\n",
    "\n",
    "- Highly efficient column-wise compression\n",
    "\n",
    "- High compatibility with with OLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot](https://github.com/DatacollectorVN/PySpark-Tutorial/blob/master/public-imgs/parquet.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save DataFrame to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Date (MM/DD/YYYY)', 'Date_MM_DD_YYYY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Flight Number', 'Flight_Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('Destination Airport', 'Destination_Airport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-------------------+---------------------------+\n",
      "|Date_MM_DD_YYYY|Flight_Number|Destination_Airport|Actual_elapsed_time_Minutes|\n",
      "+---------------+-------------+-------------------+---------------------------+\n",
      "|     01/01/2015|            5|                HNL|                        526|\n",
      "|     01/01/2015|            7|                OGG|                        517|\n",
      "|     01/01/2015|           23|                SFO|                        233|\n",
      "|     01/01/2015|           27|                LAS|                        165|\n",
      "|     01/01/2015|           29|                ONT|                          0|\n",
      "|     01/01/2015|           35|                HDN|                        178|\n",
      "|     01/01/2015|           37|                SAN|                        187|\n",
      "|     01/01/2015|           43|                DTW|                          0|\n",
      "|     01/01/2015|           49|                SAN|                        178|\n",
      "|     01/01/2015|           51|                SLC|                        161|\n",
      "|     01/01/2015|           60|                MIA|                        151|\n",
      "|     01/01/2015|           64|                JFK|                        187|\n",
      "|     01/01/2015|           71|                SAN|                        176|\n",
      "|     01/01/2015|           72|                MCO|                        142|\n",
      "|     01/01/2015|           74|                CLE|                          0|\n",
      "|     01/01/2015|           79|                SMF|                        224|\n",
      "|     01/01/2015|           81|                TUS|                        140|\n",
      "|     01/01/2015|           96|                STL|                         94|\n",
      "|     01/01/2015|          103|                MSY|                         80|\n",
      "|     01/01/2015|          119|                OGG|                        502|\n",
      "+---------------+-------------+-------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('Actual elapsed time (Minutes)', 'Actual_elapsed_time_Minutes')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(os.path.join('..', 'src-data', 'AA_DFW_ALL.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected output:* 7 partitions."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e8b2dc75a532c1b8be16e00f9f13abc240a1ac4ad83b862d6ae36ef062a2a95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
